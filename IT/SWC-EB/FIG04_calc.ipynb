{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db30292f-0923-4c2f-a0fb-cc29c7c687b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c618555a-36bf-403f-9c81-27ba0dece423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_average(arr, window=5):\n",
    "    # Compute the running average using convolution\n",
    "    conv_avg = np.convolve(arr, np.ones(window) / window, mode='valid')\n",
    "    # Prepend and append additional averages as needed\n",
    "    start_avgs = [np.mean(arr[:3]), np.mean(arr[:4])]\n",
    "    end_avgs = [np.mean(arr[-4:]), np.mean(arr[-3:])]\n",
    "    # Combine everything into one array\n",
    "    return np.concatenate((start_avgs, conv_avg, end_avgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65ed4cb8-2774-48e5-bf0a-37dc858df265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_corr_obs(rad, le, sh, gh, swc, month, samplenums=90, bootsnums=1000):\n",
    "    pairs = {   'swc_rad': ('swc', 'rad'),\n",
    "                'swc_le':  ('swc', 'le'),\n",
    "                'swc_sh':  ('swc', 'sh'),\n",
    "                'swc_gh':  ('swc', 'gh'),\n",
    "                'rad_le':  ('rad', 'le'),\n",
    "                'rad_sh':  ('rad', 'sh'),\n",
    "                'rad_gh':  ('rad', 'gh'),\n",
    "                'le_sh':   ('le', 'sh'),\n",
    "                'le_gh':   ('le', 'gh'),\n",
    "                'sh_gh':   ('sh', 'gh'),}\n",
    "    var_dict = {    'swc': swc,\n",
    "                    'rad': rad,\n",
    "                    'le': le,\n",
    "                    'sh': sh,\n",
    "                    'gh': gh}\n",
    "\n",
    "    # Create month column names as strings '1' to '12'\n",
    "    columns_mo = [str(i) for i in range(1, 13)]\n",
    "    results = {}\n",
    "    \n",
    "    # Loop over each variable pair in the dictionary.\n",
    "    for pair_name, (var1_name, var2_name) in pairs.items():\n",
    "        var1 = var_dict[var1_name]\n",
    "        var2 = var_dict[var2_name]\n",
    "        corr_values = []\n",
    "        # Loop through each month (1 to 12).\n",
    "        # len(obs)\n",
    "        for m in range(1, 13):  \n",
    "            # Filter the data corresponding to the current month.               \n",
    "            if (len(var1[month == m]) < samplenums):\n",
    "                corr_mean = np.nan\n",
    "            else:\n",
    "                bootstrap = []\n",
    "                X_m = var1[month == m]\n",
    "                Y_m = var2[month == m]               \n",
    "\n",
    "                for _ in range(bootsnums):\n",
    "                    # Sample indices with replacement\n",
    "                    idx = np.random.choice(len(X_m), samplenums, replace=True)\n",
    "                    X_sample = X_m[idx]\n",
    "                    Y_sample = Y_m[idx]\n",
    "                    \n",
    "                    # Compute Pearson correlation for the current month.\n",
    "                    corr = stats.pearsonr(X_sample, Y_sample)[0]\n",
    "                    bootstrap.append(corr)\n",
    "                corr_mean = np.mean(bootstrap)\n",
    "            corr_values.append(corr_mean)\n",
    "        # Store the monthly correlations in a DataFrame.\n",
    "        results[pair_name] = pd.DataFrame([corr_values], columns=columns_mo)\n",
    "   \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54f8f800-4759-4162-b20a-76b9b9ddacb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_RR_bins(X):\n",
    "    \"\"\"Rice Rule: k = [2 * N ** (1/3)]\"\"\"\n",
    "    num_bins = 2 * (len(X) ** (1/3))    \n",
    "    return int(num_bins)\n",
    "\n",
    "def calc_entropy(X):\n",
    "    \"\"\"Calculate the Shannon entropy of a dataset.\"\"\"\n",
    "    bins = calc_RR_bins(X)\n",
    "    # print(bins)\n",
    "    prob = np.histogram(X, bins = int(bins))[0]/len(X)    \n",
    "    return -np.sum([p * np.log2(p) for p in prob if p > 0])\n",
    "\n",
    "def calc_joint_entropy(X, Y):\n",
    "    \"\"\"Calculate the joint entropy of two datasets.\"\"\"\n",
    "    # Determine joint bins using Rice Rule\n",
    "    bins_x = calc_RR_bins(X)\n",
    "    bins_y = calc_RR_bins(Y)\n",
    "    \n",
    "    # 2D histogram for joint probability\n",
    "    hist_2d, _, _ = np.histogram2d(X, Y, bins=[bins_x, bins_y])\n",
    "    prob_2d = hist_2d / np.sum(hist_2d)\n",
    "    \n",
    "    # Calculate joint entropy\n",
    "    return -np.sum([p * np.log2(p) for p in prob_2d.flatten() if p > 0])\n",
    "\n",
    "def calc_MI(X, Y, samplenums=90, bootsnums=1000):\n",
    "    \"\"\"Calculate Mutual Information with bootstrapping.\"\"\"\n",
    "    # Filter data for the given month\n",
    "    X_m = X\n",
    "    Y_m = Y\n",
    "\n",
    "    if len(X_m) < samplenums:\n",
    "        raise ValueError(f\"Not enough samples in month {mo} to draw {samplenums} samples.\")\n",
    "    \n",
    "    mi_bootstrap = []\n",
    "\n",
    "    for _ in range(bootsnums):\n",
    "        # Sample indices with replacement\n",
    "        idx = np.random.choice(len(X_m), samplenums, replace=True)\n",
    "        X_sample = X_m[idx]\n",
    "        Y_sample = Y_m[idx]\n",
    "        \n",
    "        # Compute MI\n",
    "        H_x = calc_entropy(X_sample)\n",
    "        H_y = calc_entropy(Y_sample)\n",
    "        H_xy = calc_joint_entropy(X_sample, Y_sample)\n",
    "        I_xy = H_x + H_y - H_xy\n",
    "        # Normalize MI to [0, 1]\n",
    "        # NMI = I_xy/min(H_x, H_y)\n",
    "        NMI = I_xy/(H_x**(0.5) * H_y**(0.5))\n",
    "        \n",
    "        mi_bootstrap.append(NMI)\n",
    "        # mi_bootstrap.append(I_xy)\n",
    "        \n",
    "    # Return average MI and optionally confidence intervals\n",
    "    mi_out = np.mean(mi_bootstrap)\n",
    "    \n",
    "    return mi_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db45a4c1-ffee-439f-82b1-d0e636e66a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_z(x, y):\n",
    "    \"\"\"\n",
    "    Compute z = Y - Y^\n",
    "    \n",
    "    the least-squares regression .\n",
    "\n",
    "    Parameters:\n",
    "        x (array-like): Predictor variable (X).\n",
    "        y (array-like): Response variable (Y).\n",
    "        ye(array-like): Fitted values (Y^)\n",
    "    Returns:\n",
    "        z \n",
    "    \"\"\"\n",
    "    # Means of X and Y\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    # Calculate slope (m)\n",
    "    numerator = np.sum((x - x_mean) * (y - y_mean))\n",
    "    denominator = np.sum((x - x_mean)**2)\n",
    "    slope = numerator / denominator\n",
    "    \n",
    "    # Calculate intercept (b)\n",
    "    intercept = y_mean - slope * x_mean\n",
    "    \n",
    "    # Calculate fitted values Y^\n",
    "    fitted_values = slope * np.array(x) + intercept\n",
    "    \n",
    "    return fitted_values - np.array(y), fitted_values\n",
    "    \n",
    "    # return fitted_values\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "def calc_cdf(x):\n",
    "    x_mean = np.mean(x)\n",
    "    x_std  = np.std(x)\n",
    "\n",
    "    return norm.cdf(x , loc=x_mean, scale=x_std)\n",
    "\n",
    "def calc_quantile_transform(x,y):\n",
    "    \"\"\"\n",
    "    Calculate Y' = F_Y^{-1}(G(z)).\n",
    "    Smith (2015)\n",
    "    Parameters:\n",
    "        x (array-like): Predictor variable (X).\n",
    "        y (array-like): Response variable (Y).\n",
    "        z (array-like): Residual values of Y-Y^.\n",
    "    Returns:\n",
    "        y_p (array-like): Quantile function (inverse c.d.f.) of G(z)\n",
    "    \"\"\"   \n",
    "    # c.d.f of G(z)\n",
    "    z     = calc_z (x,y)\n",
    "    cdf_z = calc_cdf(z[0])\n",
    "    data = np.sort(y)\n",
    "    n   = len(data)\n",
    "    ind = (cdf_z*(n -1)).astype(int)\n",
    "    # quantile function (inverse c.d.f.) of G(z)\n",
    "    y_p = data[ind]\n",
    "    \n",
    "    return y_p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9750c6ba-8456-48a2-af75-990b798f1568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_joint_entropy3(X, Y, Z):\n",
    "    \"\"\"\n",
    "    Calculate the joint entropy of three variables.\n",
    "    \n",
    "    Parameters:\n",
    "        X, Y, Z: Array-like input variables (same length).\n",
    "    \n",
    "    Returns:\n",
    "        float: Joint entropy H(X, Y, Z).\n",
    "    \"\"\"\n",
    "    # Determine bins for each variable\n",
    "    bins_x = calc_RR_bins(X)\n",
    "    bins_y = calc_RR_bins(Y)\n",
    "    bins_z = calc_RR_bins(Z)\n",
    "    \n",
    "    # 3D histogram for joint probability\n",
    "    hist_3d, edges = np.histogramdd(np.vstack((X, Y, Z)).T, bins=[bins_x, bins_y, bins_z])\n",
    "    prob_3d = hist_3d / np.sum(hist_3d)  # Normalize to get probabilities\n",
    "    \n",
    "    # Calculate joint entropy\n",
    "    joint_entropy = -np.sum([p * np.log2(p) for p in prob_3d.flatten() if p > 0])\n",
    "    return joint_entropy\n",
    "    \n",
    "def calc_transfer_entropy(X, Y, lag=1):\n",
    "    \"\"\"\n",
    "    Calculate the transfer entropy T(X -> Y).\n",
    "    \n",
    "    Parameters:\n",
    "        X (array-like): Source time series.\n",
    "        Y (array-like): Target time series.\n",
    "        lag (int): Time lag for past states.\n",
    "    \n",
    "    Returns:\n",
    "        float: Transfer entropy T(X -> Y).\n",
    "    \"\"\"\n",
    "    # Lagged time series\n",
    "    x_t = X[:-lag]\n",
    "    y_t = Y[:-lag]\n",
    "    y_t1 = Y[lag:]  # Future values of Y\n",
    "    \n",
    "    # Calculate individual and joint entropies\n",
    "    H_yt1_yt_xt = calc_joint_entropy3(y_t1, y_t, x_t)\n",
    "    H_yt_yt_xt = calc_joint_entropy3(y_t, y_t, x_t)\n",
    "    H_yt1_yt = calc_joint_entropy(y_t1, y_t)\n",
    "    H_yt = calc_entropy(y_t)\n",
    "    \n",
    "    # Transfer Entropy: T(X -> Y)\n",
    "    TE = -(H_yt1_yt_xt - H_yt_yt_xt - (H_yt1_yt - H_yt))\n",
    "    return TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36be18ac-7cef-48b1-8461-3fcd7151c17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_MI_obs(rad, le, sh, gh, swc, month, samplenums=90):\n",
    "    pairs = {   'swc_rad': ('swc', 'rad'),\n",
    "                'swc_le':  ('swc', 'le'),\n",
    "                'swc_sh':  ('swc', 'sh'),\n",
    "                'swc_gh':  ('swc', 'gh'),\n",
    "                'rad_le':  ('rad', 'le'),\n",
    "                'rad_sh':  ('rad', 'sh'),\n",
    "                'rad_gh':  ('rad', 'gh'),\n",
    "                'le_sh':   ('le', 'sh'),\n",
    "                'le_gh':   ('le', 'gh'),\n",
    "                'sh_gh':   ('sh', 'gh'),}\n",
    "    var_dict = {    'swc': swc,\n",
    "                    'rad': rad,\n",
    "                    'le': le,\n",
    "                    'sh': sh,\n",
    "                    'gh': gh}\n",
    "\n",
    "    # Create month column names as strings '1' to '12'\n",
    "    columns_mo = [str(i) for i in range(1, 13)]\n",
    "    results1 = {}\n",
    "    results2 = {}\n",
    "    results3 = {}\n",
    "    results4 = {}\n",
    "    results5 = {}\n",
    "    \n",
    "    # Loop over each variable pair in the dictionary.\n",
    "    for pair_name, (var1_name, var2_name) in pairs.items():\n",
    "        var1 = var_dict[var1_name]\n",
    "        var2 = var_dict[var2_name]\n",
    "        nmi_values = []\n",
    "        nmi_nonlinear_values = []\n",
    "        nmi_linear_values = []\n",
    "        te_x_y_values = []\n",
    "        te_y_x_values = []\n",
    "        \n",
    "        # Loop through each month (1 to 12).\n",
    "        # len(obs)\n",
    "        for m in range(1, 13):  \n",
    "            # Filter the data corresponding to the current month.               \n",
    "            if (len(var1[month == m]) < samplenums):\n",
    "                nmi = np.nan\n",
    "                nmi_nonlinear = np.nan\n",
    "                nmi_linear = np.nan\n",
    "                te_x_y = np.nan\n",
    "                te_y_x = np.nan\n",
    "            else:\n",
    "                # Compute Pearson correlation for the current month.\n",
    "                X  = var1[month==m]\n",
    "                Y  = var2[month==m]\n",
    "                # print(X)\n",
    "                nmi = calc_MI(X,Y)\n",
    "                Y_p= calc_quantile_transform(X,Y)\n",
    "                nmi_nonlinear = calc_MI(X,Y_p)\n",
    "                nmi_linear    = nmi - nmi_nonlinear\n",
    "                te_x_y        = calc_transfer_entropy(X,Y)\n",
    "                te_y_x        = calc_transfer_entropy(Y,X)\n",
    "                \n",
    "            nmi_values.append(nmi)\n",
    "            nmi_nonlinear_values.append(nmi_nonlinear)\n",
    "            nmi_linear_values.append(nmi_linear)\n",
    "            te_x_y_values.append(te_x_y)\n",
    "            te_y_x_values.append(te_y_x)\n",
    "            \n",
    "        # Store the monthly correlations in a DataFrame.\n",
    "        results1[pair_name] = pd.DataFrame([nmi_values], columns=columns_mo)\n",
    "        results2[pair_name] = pd.DataFrame([nmi_nonlinear_values], columns=columns_mo)\n",
    "        results3[pair_name] = pd.DataFrame([nmi_linear_values], columns=columns_mo)\n",
    "        results4[pair_name] = pd.DataFrame([te_x_y_values], columns=columns_mo)\n",
    "        results5[pair_name] = pd.DataFrame([te_y_x_values], columns=columns_mo)\n",
    "           \n",
    "    return results1,results2,results3,results4,results5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1557df76-7ce5-4bb0-a30e-0ea269f05aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_obs = ['US-A03', 'US-A10', 'US-A32', 'US-A74', 'US-ADR', 'US-AR1', 'US-AR2', 'US-ARM', 'US-BMM', 'US-Bi1', 'US-Bi2', 'US-Bo1', 'US-Bo2', 'US-Bsg', 'US-CGG', 'US-CPk', 'US-CRT', 'US-CS2', 'US-Ced', 'US-Cst', 'US-DFC', 'US-DFK', 'US-Dia', 'US-Dix', 'US-Elm', 'US-Esm', 'US-Fcr', 'US-Fmf', 'US-Fuf', 'US-Fwf', 'US-GBT', 'US-GLE', 'US-HB2', 'US-HB3', 'US-HBK', 'US-HRA', 'US-HRC', 'US-HWB', 'US-Hn2', 'US-Hn3', 'US-Ho1', 'US-Ho3', 'US-IB1', 'US-IB2', 'US-ICh', 'US-ICs', 'US-ICt', 'US-Jo1', 'US-Jo2', 'US-KFS', 'US-KLS', 'US-KM4', 'US-KUT', 'US-Kon', 'US-Lin', 'US-MC1', 'US-MC2', 'US-MH1', 'US-MH2', 'US-MN3', 'US-MOz', 'US-MSR', 'US-MVW', 'US-Me1', 'US-Me2', 'US-Me5', 'US-Me6', 'US-Mi1', 'US-Mi2', 'US-Mi3', 'US-Mj1', 'US-Mj2', 'US-Mo1', 'US-Mo2', 'US-Mo3', 'US-MtB', 'US-NC1', 'US-NC2', 'US-NC3', 'US-NC4', 'US-NGC', 'US-NR3', 'US-NR4', 'US-ONA', 'US-Oho', 'US-PFL', 'US-PFb', 'US-PFc', 'US-PFd', 'US-PFe', 'US-PFf', 'US-PFg', 'US-PFh', 'US-PFi', 'US-PFj', 'US-PFk', 'US-PFm', 'US-PFn', 'US-PFp', 'US-PFq', 'US-PFr', 'US-PFs', 'US-PFt', 'US-Rls', 'US-Rms', 'US-Ro1', 'US-Ro4', 'US-Ro5', 'US-Ro6', 'US-Rpf', 'US-Rws', 'US-SP1', 'US-SP2', 'US-SP3', 'US-SRC', 'US-SRG', 'US-SRS', 'US-SdH', 'US-Slt', 'US-Snd', 'US-Syv', 'US-Ton', 'US-Tur', 'US-Tw2', 'US-Tw3', 'US-UC1', 'US-UC2', 'US-UiA', 'US-UiB', 'US-UiC', 'US-UiD', 'US-Var', 'US-WCr', 'US-Whs', 'US-Wkg', 'US-Wlr', 'US-Wrc', 'US-YK1', 'US-YK2', 'US-xAB', 'US-xAE', 'US-xBA', 'US-xBL', 'US-xBN', 'US-xBR', 'US-xCL', 'US-xCP', 'US-xDC', 'US-xDJ', 'US-xDL', 'US-xDS', 'US-xGR', 'US-xHA', 'US-xHE', 'US-xJE', 'US-xJR', 'US-xKA', 'US-xKZ', 'US-xLE', 'US-xMB', 'US-xML', 'US-xNG', 'US-xNQ', 'US-xNW', 'US-xRM', 'US-xRN', 'US-xSB', 'US-xSC', 'US-xSE', 'US-xSJ', 'US-xSL', 'US-xSP', 'US-xSR', 'US-xST', 'US-xTA', 'US-xTE', 'US-xTL', 'US-xTR', 'US-xUK', 'US-xUN', 'US-xWD', 'US-xWR', 'US-xYE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2eb9b8ab-2ce5-4658-884d-433f72fa544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def mk_obs_mo_r(time_scale):\n",
    "    dir = '/projects/COLA/land/skim/FLUXNET/AMERIbase/out_v2/'\n",
    "    site_list = site_obs\n",
    "    f_start = 0 ; f_stop = len(site_list)\n",
    "    \n",
    "    columns = ['NETRAD_1_A','LE_1_A','H_1_A','G_1_A','SWC_1_A']\n",
    "    pairs = {   'swc_rad': ('swc', 'rad'),\n",
    "                'swc_le':  ('swc', 'le'),\n",
    "                'swc_sh':  ('swc', 'sh'),\n",
    "                'swc_gh':  ('swc', 'gh'),\n",
    "                'rad_le':  ('rad', 'le'),\n",
    "                'rad_sh':  ('rad', 'sh'),\n",
    "                'rad_gh':  ('rad', 'gh'),\n",
    "                'le_sh':   ('le', 'sh'),\n",
    "                'le_gh':   ('le', 'gh'),\n",
    "                'sh_gh':   ('sh', 'gh'),}\n",
    "    \n",
    "    columns_mo   = [str(i) for i in range(1, 13)]\n",
    "    results_out  = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out2 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out3 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out4 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out5 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out6 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "     \n",
    "    for s,site in enumerate(site_list[0:f_stop]):\n",
    "    # for s,site in enumerate(site_list[0:5]):\n",
    "        f_siteid = site_list[s]\n",
    "        # print(f_siteid)\n",
    "        df   = pd.read_csv(dir+f_siteid+'.csv',na_values=-9999)\n",
    "        dir_loc = '/projects/COLA/land/skim/FLUXNET/AMERIbase/out_v2/site/'\n",
    "        df_loc  = pd.read_csv(dir_loc+f_siteid+'_site.csv',na_values=-9999)\n",
    "        \n",
    "        lat     = df_loc['Latitude'].item()\n",
    "        if float(lat) < 50:                \n",
    "            tm      = df['TIME']    \n",
    "            yrs     = str(tm[0])[:4]   ;    yre     = str(tm[int(len(tm)-1)])[:4]\n",
    "            mos     = str(tm[0])[4:6]  ;    moe     = str(tm[int(len(tm)-1)])[4:6]\n",
    "            dys     = str(tm[0])[6:8]  ;    dye     = str(tm[int(len(tm)-1)])[6:8]\n",
    "            hrs     = str(tm[0])[8:10] ;    hre     = str(tm[int(len(tm)-1)])[8:10]\n",
    "            mns     = str(tm[0])[10:12];    mne     = str(tm[int(len(tm)-1)])[10:12]\n",
    "\n",
    "            time = pd.date_range(yrs+'-'+mos+'-'+dys+' '+hrs+':'+mns,yre+'-'+moe+'-'+dye+' '+hre+':'+mne,freq='30min')        \n",
    "            df.index = pd.to_datetime(time)\n",
    "            \n",
    "            rad    = running_average(df[columns[0]].resample(time_scale).mean())   \n",
    "            le     = running_average(df[columns[1]].resample(time_scale).mean())\n",
    "            sh     = running_average(df[columns[2]].resample(time_scale).mean())\n",
    "            gh     = running_average(df[columns[3]].resample(time_scale).mean())\n",
    "            swc    = running_average(df[columns[4]].resample(time_scale).mean())\n",
    "\n",
    "            time_dy = pd.date_range(yrs+'-'+mos+'-'+dys+' '+hrs+':'+mns,yre+'-'+moe+'-'+dye+' '+hre+':'+mne,freq='1d')        \n",
    "\n",
    "            rad = pd.Series(rad, index=pd.to_datetime(time_dy))\n",
    "            le  = pd.Series(le, index=pd.to_datetime(time_dy))\n",
    "            sh  = pd.Series(sh, index=pd.to_datetime(time_dy))\n",
    "            gh  = pd.Series(gh, index=pd.to_datetime(time_dy))\n",
    "            swc = pd.Series(swc, index=pd.to_datetime(time_dy))\n",
    "            # # no running avg option\n",
    "            # rad    = df[columns[0]].resample(time_scale).mean()  \n",
    "            # le     = df[columns[1]].resample(time_scale).mean()  \n",
    "            # sh     = df[columns[2]].resample(time_scale).mean()  \n",
    "            # gh     = df[columns[3]].resample(time_scale).mean()  \n",
    "            # swc    = df[columns[4]].resample(time_scale).mean()  \n",
    "            \n",
    "            \n",
    "            sort_nan = ~np.logical_or.reduce((np.isnan(rad),np.isnan(le),np.isnan(sh),np.isnan(gh),np.isnan(swc)))\n",
    "\n",
    "            month = rad.index.month[sort_nan]\n",
    "            rad   = rad[sort_nan]\n",
    "            le    = le [sort_nan]\n",
    "            sh    = sh [sort_nan]\n",
    "            gh    = gh [sort_nan]\n",
    "            swc   = swc[sort_nan]\n",
    "\n",
    "            if int(yre) > 2020:\n",
    "                month = month[rad.index.year <= 2020]\n",
    "                rad   = rad[rad.index.year <= 2020]\n",
    "                le    = le [le.index.year <= 2020]\n",
    "                sh    = sh [sh.index.year <= 2020]\n",
    "                gh    = gh [gh.index.year <= 2020]\n",
    "                swc   = swc[swc.index.year <= 2020]\n",
    "\n",
    "            if int(yrs) < 2021:\n",
    "                # corr = mk_corr_obs(rad, le, sh, gh, swc, month)\n",
    "                mi   = mk_MI_obs(rad, le, sh, gh, swc, month)\n",
    "                # print(mi[0])\n",
    "            for pair_name in pairs:\n",
    "                # results_out [pair_name] = pd.concat([results_out [pair_name], corr[pair_name]],ignore_index=True)\n",
    "                results_out2[pair_name] = pd.concat([results_out2[pair_name], mi[0][pair_name]],ignore_index=True)\n",
    "                results_out3[pair_name] = pd.concat([results_out3[pair_name], mi[1][pair_name]],ignore_index=True)\n",
    "                results_out4[pair_name] = pd.concat([results_out4[pair_name], mi[2][pair_name]],ignore_index=True)\n",
    "                results_out5[pair_name] = pd.concat([results_out5[pair_name], mi[3][pair_name]],ignore_index=True)\n",
    "                results_out6[pair_name] = pd.concat([results_out6[pair_name], mi[4][pair_name]],ignore_index=True)\n",
    "                \n",
    "\n",
    "    print('done')\n",
    "    return  results_out2,results_out3,results_out4,results_out5,results_out6\n",
    "    # return  results_out, results_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6547b848-88be-4e5e-a486-275955b89f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "obs = mk_obs_mo_r('1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48bc7cde-4f4e-4b4c-82ac-ce247eb8dff9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mk_sort_nan(site):\n",
    "    dir     = '/projects/COLA/land/skim/FLUXNET/AMERIbase/out_v2/'\n",
    "    # dir     = '/projects/COLA/land/skim/ameribase_skim/'\n",
    "    columns = ['NETRAD_1_A','LE_1_A','H_1_A','G_1_A','SWC_1_A']\n",
    "    df   = pd.read_csv(dir+site+'.csv',na_values=-9999)    \n",
    "        \n",
    "    tm      = df['TIME']    \n",
    "    yrs     = str(tm[0])[:4]   ;    yre     = str(tm[int(len(tm)-1)])[:4]\n",
    "    mos     = str(tm[0])[4:6]  ;    moe     = str(tm[int(len(tm)-1)])[4:6]\n",
    "    dys     = str(tm[0])[6:8]  ;    dye     = str(tm[int(len(tm)-1)])[6:8]\n",
    "    hrs     = str(tm[0])[8:10] ;    hre     = str(tm[int(len(tm)-1)])[8:10]\n",
    "    mns     = str(tm[0])[10:12];    mne     = str(tm[int(len(tm)-1)])[10:12]\n",
    "\n",
    "    time = pd.date_range(yrs+'-'+mos+'-'+dys+' '+hrs+':'+mns,yre+'-'+moe+'-'+dye+' '+hre+':'+mne,freq='30MIN')        \n",
    "    df.index = pd.to_datetime(time)\n",
    "\n",
    "    rad    = df[columns[0]].resample('1D').mean()    \n",
    "    le     = df[columns[1]].resample('1D').mean()\n",
    "    sh     = df[columns[2]].resample('1D').mean()\n",
    "    gh     = df[columns[3]].resample('1D').mean()\n",
    "    swc1   = df[columns[4]].resample('1D').mean()\n",
    "\n",
    "    if int(yre) > 2020:            \n",
    "        rad = rad[rad.index.year <= 2020]\n",
    "        le  = le [le.index.year <= 2020]\n",
    "        sh  = sh [sh.index.year <= 2020]\n",
    "        gh  = gh [gh.index.year <= 2020]\n",
    "        swc1= swc1[swc1.index.year <= 2020]\n",
    "\n",
    "        \n",
    "    sort_nan = ~np.logical_or.reduce((np.isnan(rad),np.isnan(le),np.isnan(sh),np.isnan(gh),np.isnan(swc1)))\n",
    "    \n",
    "    return sort_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67de75f0-64ce-42dd-a4a8-229de25d5490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_era_mo_r(time_scale):\n",
    "    site_list = site_obs\n",
    "    f_start = 0 ; f_stop = len(site_list)\n",
    "    \n",
    "    columns = ['NETRAD_1_A','LE_1_A','H_1_A','G_1_A','SWC_1_A']\n",
    "    pairs = {   'swc_rad': ('swc', 'rad'),\n",
    "                'swc_le':  ('swc', 'le'),\n",
    "                'swc_sh':  ('swc', 'sh'),\n",
    "                'swc_gh':  ('swc', 'gh'),\n",
    "                'rad_le':  ('rad', 'le'),\n",
    "                'rad_sh':  ('rad', 'sh'),\n",
    "                'rad_gh':  ('rad', 'gh'),\n",
    "                'le_sh':   ('le', 'sh'),\n",
    "                'le_gh':   ('le', 'gh'),\n",
    "                'sh_gh':   ('sh', 'gh'),}\n",
    "    \n",
    "    columns_mo = [str(i) for i in range(1, 13)]\n",
    "    results_out = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out2 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out3 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out4 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out5 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out6 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "     \n",
    "    for s,site in enumerate(site_list[:]):\n",
    "    # for s,site in enumerate(site_list[8:9]):\n",
    "        f_siteid = site_list[s]\n",
    "        \n",
    "        dir_loc = '/projects/COLA/land/skim/FLUXNET/AMERIbase/out_v2/site/'\n",
    "        df_loc  = pd.read_csv(dir_loc+f_siteid+'_site.csv',na_values=-9999)\n",
    "        \n",
    "        lat     = df_loc['Latitude'].item()\n",
    "        if float(lat) < 50:\n",
    "            dir1  = '/projects/COLA/land/skim/FLUXNET/AMERIbase/era5/out_v2'\n",
    "            df1   = pd.read_csv(dir1+'/LE_1_A/'+f_siteid+'.csv',na_values=-9999)\n",
    "            df2   = pd.read_csv(dir1+'/H_1_A/'+f_siteid+'.csv',na_values=-9999)\n",
    "            # df3   = pd.read_csv(dir1+'/GH_1_A/'+f_siteid+'.csv',na_values=-9999)\n",
    "            df4   = pd.read_csv(dir1+'/SWC_1_A/'+f_siteid+'.csv',na_values=-9999)\n",
    "            df5   = pd.read_csv(dir1+'/SW_1_A/'+f_siteid+'.csv',na_values=-9999)\n",
    "            df6   = pd.read_csv(dir1+'/LW_1_A/'+f_siteid+'.csv',na_values=-9999)\n",
    "\n",
    "            tm      = df1['TIME']    \n",
    "            yrs     = str(tm[0])[:4]   ;    yre     = str(tm[int(len(tm)-1)])[:4]\n",
    "            mos     = str(tm[0])[5:7]  ;    moe     = str(tm[int(len(tm)-1)])[5:7]\n",
    "            dys     = str(tm[0])[8:10]  ;    dye     = str(tm[int(len(tm)-1)])[8:10]\n",
    "            hrs     = str(tm[0])[11:13] ;    hre     = str(tm[int(len(tm)-1)])[11:13]\n",
    "            mns     = str(tm[0])[14:16];    mne     = str(tm[int(len(tm)-1)])[14:16]\n",
    "            # print(f_siteid,yrs, mos, yre, moe)\n",
    "\n",
    "            time  = pd.date_range(yrs+'-'+mos+'-'+dys,yre+'-'+moe+'-'+dye,freq='1D')   \n",
    "            df1.index = pd.to_datetime(time)\n",
    "            df2.index = pd.to_datetime(time)\n",
    "            # df3.index = pd.to_datetime(time)\n",
    "            df4.index = pd.to_datetime(time)\n",
    "            df5.index = pd.to_datetime(time)\n",
    "            df6.index = pd.to_datetime(time)  \n",
    "\n",
    "            le   = df1['LE_1_A']\n",
    "            sh   = df2['H_1_A']\n",
    "            # gh   = df3['GH_1_A'][sort_nan]\n",
    "            swc  = df4['SWC_1_A']\n",
    "            sw   = df5['SW_1_A']\n",
    "            lw   = df6['LW_1_A']\n",
    "            rad  = sw + lw\n",
    "            gh   = rad - (le + sh)\n",
    "\n",
    "            rad    = running_average(rad)   \n",
    "            le     = running_average(le)\n",
    "            sh     = running_average(sh)\n",
    "            gh     = running_average(gh)\n",
    "            swc    = running_average(swc)\n",
    "\n",
    "            rad = pd.Series(rad, index=pd.to_datetime(time))\n",
    "            le  = pd.Series(le, index=pd.to_datetime(time))\n",
    "            sh  = pd.Series(sh, index=pd.to_datetime(time))\n",
    "            gh  = pd.Series(gh, index=pd.to_datetime(time))\n",
    "            swc = pd.Series(swc, index=pd.to_datetime(time))\n",
    "\n",
    "            month = le.index.month\n",
    "            sort_nan = mk_sort_nan(f_siteid)\n",
    "\n",
    "            if (len(le.values) > len(sort_nan)):\n",
    "                min_len = min(len(month.values), len(sort_nan))\n",
    "                month = month[:min_len]\n",
    "                rad   = rad[:min_len]\n",
    "                le    = le [:min_len]\n",
    "                sh    = sh [:min_len]\n",
    "                gh    = gh [:min_len]\n",
    "                swc   = swc[:min_len]                \n",
    "\n",
    "                month = month[sort_nan]\n",
    "                rad   = rad[sort_nan]\n",
    "                le    = le [sort_nan]\n",
    "                sh    = sh [sort_nan]\n",
    "                gh    = gh [sort_nan]\n",
    "                swc   = swc[sort_nan]\n",
    "\n",
    "                del min_len\n",
    "            else:\n",
    "                month = month[sort_nan]\n",
    "                rad   = rad[sort_nan]\n",
    "                le    = le [sort_nan]\n",
    "                sh    = sh [sort_nan]\n",
    "                gh    = gh [sort_nan]\n",
    "                swc   = swc[sort_nan]\n",
    "\n",
    "            # corr = mk_corr_obs(rad, le, sh, gh, swc, month)\n",
    "            mi   = mk_MI_obs(rad, le, sh, gh, swc, month)\n",
    "                \n",
    "            for pair_name in pairs:\n",
    "                # results_out[pair_name] = pd.concat([results_out[pair_name], corr[pair_name]],ignore_index=True)\n",
    "                results_out2[pair_name] = pd.concat([results_out2[pair_name], mi[0][pair_name]],ignore_index=True)\n",
    "                results_out3[pair_name] = pd.concat([results_out3[pair_name], mi[1][pair_name]],ignore_index=True)\n",
    "                results_out4[pair_name] = pd.concat([results_out4[pair_name], mi[2][pair_name]],ignore_index=True)\n",
    "                results_out5[pair_name] = pd.concat([results_out5[pair_name], mi[3][pair_name]],ignore_index=True)\n",
    "                results_out6[pair_name] = pd.concat([results_out6[pair_name], mi[4][pair_name]],ignore_index=True)\n",
    "                \n",
    "\n",
    "    print('done')\n",
    "    return  results_out2,results_out3,results_out4,results_out5,results_out6\n",
    "    # return  results_out, results_out2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01681ae1-18fb-43f0-ac20-0fec70935014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "era = mk_era_mo_r('1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07934b08-4a43-449a-96c6-6a4b1cf94a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_merra_mo_r(time_scale):\n",
    "    site_list = site_obs\n",
    "    f_start = 0 ; f_stop = len(site_list)\n",
    "    \n",
    "    columns = ['NETRAD_1_A','LE_1_A','H_1_A','G_1_A','SWC_1_A']\n",
    "    pairs = {   'swc_rad': ('swc', 'rad'),\n",
    "                'swc_le':  ('swc', 'le'),\n",
    "                'swc_sh':  ('swc', 'sh'),\n",
    "                'swc_gh':  ('swc', 'gh'),\n",
    "                'rad_le':  ('rad', 'le'),\n",
    "                'rad_sh':  ('rad', 'sh'),\n",
    "                'rad_gh':  ('rad', 'gh'),\n",
    "                'le_sh':   ('le', 'sh'),\n",
    "                'le_gh':   ('le', 'gh'),\n",
    "                'sh_gh':   ('sh', 'gh'),}\n",
    "    \n",
    "    columns_mo = [str(i) for i in range(1, 13)]\n",
    "    results_out = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out2 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out3 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out4 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out5 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out6 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "       \n",
    "    for s,site in enumerate(site_list[:]):\n",
    "    # for s,site in enumerate(site_list[8:9]):\n",
    "        f_siteid = site_list[s]\n",
    "        \n",
    "        dir_loc = '/projects/COLA/land/skim/FLUXNET/AMERIbase/out_v2/site/'\n",
    "        df_loc  = pd.read_csv(dir_loc+f_siteid+'_site.csv',na_values=-9999)\n",
    "        \n",
    "        lat     = df_loc['Latitude'].item()\n",
    "        if float(lat) < 50:\n",
    "            dir1  = '/projects/COLA/land/skim/FLUXNET/AMERIbase/merra2/out_v2'\n",
    "            df1   = pd.read_csv(dir1+'/LE_1_A/'+f_siteid+'.csv',na_values=-9999)\n",
    "            df2   = pd.read_csv(dir1+'/H_1_A/'+f_siteid+'.csv',na_values=-9999)\n",
    "            df3   = pd.read_csv(dir1+'/GH_1_A/'+f_siteid+'.csv',na_values=-9999)\n",
    "            df4   = pd.read_csv(dir1+'/SWC_1_A/'+f_siteid+'.csv',na_values=-9999)\n",
    "            df5   = pd.read_csv(dir1+'/SW_1_A/'+f_siteid+'.csv',na_values=-9999)\n",
    "            df6   = pd.read_csv(dir1+'/LW_1_A/'+f_siteid+'.csv',na_values=-9999)\n",
    "\n",
    "            tm      = df1['TIME']    \n",
    "            yrs     = str(tm[0])[:4]   ;    yre     = str(tm[int(len(tm)-1)])[:4]\n",
    "            mos     = str(tm[0])[5:7]  ;    moe     = str(tm[int(len(tm)-1)])[5:7]\n",
    "            dys     = str(tm[0])[8:10]  ;    dye     = str(tm[int(len(tm)-1)])[8:10]\n",
    "            hrs     = str(tm[0])[11:13] ;    hre     = str(tm[int(len(tm)-1)])[11:13]\n",
    "            mns     = str(tm[0])[14:16];    mne     = str(tm[int(len(tm)-1)])[14:16]\n",
    "            # print(f_siteid,yrs, mos, yre, moe)\n",
    "\n",
    "            time  = pd.date_range(yrs+'-'+mos+'-'+dys,yre+'-'+moe+'-'+dye,freq='1D')   \n",
    "            df1.index = pd.to_datetime(time)\n",
    "            df2.index = pd.to_datetime(time)\n",
    "            df3.index = pd.to_datetime(time)\n",
    "            df4.index = pd.to_datetime(time)\n",
    "            df5.index = pd.to_datetime(time)\n",
    "            df6.index = pd.to_datetime(time)  \n",
    "\n",
    "            if (df1.columns[1] == 'GH_1_A'):\n",
    "                le   = df1['GH_1_A']\n",
    "            else:\n",
    "                le   = df1['LE_1_A']\n",
    "\n",
    "            if (df2.columns[1] == 'GH_1_A'):\n",
    "                sh   = df2['GH_1_A']\n",
    "            else:\n",
    "                sh   = df2['H_1_A']\n",
    "\n",
    "            if (df4.columns[1] == 'GH_1_A'):\n",
    "                swc   = df4['GH_1_A']\n",
    "            else:\n",
    "                swc   = df4['SWC_1_A']\n",
    "\n",
    "            if (df5.columns[1] == 'GH_1_A'):\n",
    "                sw   = df5['GH_1_A']\n",
    "            else:\n",
    "                sw   = df5['SW_1_A']\n",
    "\n",
    "            if (df6.columns[1] == 'GH_1_A'):\n",
    "                lw   = df6['GH_1_A']\n",
    "            else:\n",
    "                lw   = df6['LW_1_A']\n",
    "\n",
    "            gh   = df3['GH_1_A']\n",
    "            rad  = sw + lw\n",
    "            # gh   = rad - (le + sh)\n",
    "            rad    = running_average(rad)   \n",
    "            le     = running_average(le)\n",
    "            sh     = running_average(sh)\n",
    "            gh     = running_average(gh)\n",
    "            swc    = running_average(swc)\n",
    "\n",
    "            rad = pd.Series(rad, index=pd.to_datetime(time))\n",
    "            le  = pd.Series(le, index=pd.to_datetime(time))\n",
    "            sh  = pd.Series(sh, index=pd.to_datetime(time))\n",
    "            gh  = pd.Series(gh, index=pd.to_datetime(time))\n",
    "            swc = pd.Series(swc, index=pd.to_datetime(time))\n",
    "\n",
    "            month = le.index.month\n",
    "            sort_nan = mk_sort_nan(f_siteid)\n",
    "\n",
    "\n",
    "            if (len(le.values) > len(sort_nan)):\n",
    "                min_len = min(len(month.values), len(sort_nan))\n",
    "                month = month[:min_len]\n",
    "                rad   = rad[:min_len]\n",
    "                le    = le [:min_len]\n",
    "                sh    = sh [:min_len]\n",
    "                gh    = gh [:min_len]\n",
    "                swc   = swc[:min_len]                \n",
    "\n",
    "                month = month[sort_nan]\n",
    "                rad   = rad[sort_nan]\n",
    "                le    = le [sort_nan]\n",
    "                sh    = sh [sort_nan]\n",
    "                gh    = gh [sort_nan]\n",
    "                swc   = swc[sort_nan]\n",
    "\n",
    "                del min_len\n",
    "            else:\n",
    "                month = month[sort_nan]\n",
    "                rad   = rad[sort_nan]\n",
    "                le    = le [sort_nan]\n",
    "                sh    = sh [sort_nan]\n",
    "                gh    = gh [sort_nan]\n",
    "                swc   = swc[sort_nan]\n",
    "    \n",
    "            # corr = mk_corr_obs(rad, le, sh, gh, swc, month)\n",
    "            mi   = mk_MI_obs(rad, le, sh, gh, swc, month)\n",
    "                \n",
    "            for pair_name in pairs:\n",
    "                # results_out[pair_name] = pd.concat([results_out[pair_name], corr[pair_name]],ignore_index=True)\n",
    "                results_out2[pair_name] = pd.concat([results_out2[pair_name], mi[0][pair_name]],ignore_index=True)\n",
    "                results_out3[pair_name] = pd.concat([results_out3[pair_name], mi[1][pair_name]],ignore_index=True)\n",
    "                results_out4[pair_name] = pd.concat([results_out4[pair_name], mi[2][pair_name]],ignore_index=True)\n",
    "                results_out5[pair_name] = pd.concat([results_out5[pair_name], mi[3][pair_name]],ignore_index=True)\n",
    "                results_out6[pair_name] = pd.concat([results_out6[pair_name], mi[4][pair_name]],ignore_index=True)\n",
    "                \n",
    "\n",
    "    print('done')\n",
    "    return  results_out2,results_out3,results_out4,results_out5,results_out6\n",
    "    # return  results_out, results_out2   \n",
    "    # print('done')\n",
    "    # return results_out,results_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55bed105-f053-430a-81ac-0ca6a0178019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "merra = mk_merra_mo_r('1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e0af47cf-e9f3-4f63-bfbf-0f989d23b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# def corr_mo(var1, var2, month, mo):\n",
    "#     # 99% confidence lv. p 0.01\n",
    "#     # 95% confidence lv. p 0.03\n",
    "#     # 95% confidence lv. p 0.1\n",
    "    \n",
    "#     # if (stats.pearsonr(var1[month == mo],var2[month == mo])[1] <= 0.01):\n",
    "#     corr= stats.pearsonr(var1[month == mo],var2[month == mo])[0]\n",
    "#     # else:            \n",
    "#         # corr= np.nan\n",
    "        \n",
    "#     return corr\n",
    "\n",
    "def mk_corr_cesm(s, rad, le, sh, gh, swc, month, samplenums=90, bootsnums=1000):\n",
    "    pairs = {   'swc_rad': ('swc', 'rad'),\n",
    "                'swc_le':  ('swc', 'le'),\n",
    "                'swc_sh':  ('swc', 'sh'),\n",
    "                'swc_gh':  ('swc', 'gh'),\n",
    "                'rad_le':  ('rad', 'le'),\n",
    "                'rad_sh':  ('rad', 'sh'),\n",
    "                'rad_gh':  ('rad', 'gh'),\n",
    "                'le_sh':   ('le', 'sh'),\n",
    "                'le_gh':   ('le', 'gh'),\n",
    "                'sh_gh':   ('sh', 'gh'),}\n",
    "    var_dict = {    'swc': swc,\n",
    "                    'rad': rad,\n",
    "                    'le': le,\n",
    "                    'sh': sh,\n",
    "                    'gh': gh}\n",
    "\n",
    "    # Create month column names as strings '1' to '12'\n",
    "    columns_mo = [str(i) for i in range(1, 13)]\n",
    "    results = {}\n",
    "    dir_obs = '/projects/COLA/land/skim/FLUXNET/AMERIbase/'\n",
    "    obs     = pd.read_csv(dir_obs+'obs_chck.csv',na_values=-9999)\n",
    "    \n",
    "    # Loop over each variable pair in the dictionary.\n",
    "    for pair_name, (var1_name, var2_name) in pairs.items():\n",
    "        df1 = var_dict[var1_name].reset_index()\n",
    "        df2 = var_dict[var2_name].reset_index()\n",
    "\n",
    "        corr_values = []\n",
    "        month_values = []\n",
    "\n",
    "        df1['TIME']  = df1['TIME'].astype(str)\n",
    "        df1['MONTH'] = df1['TIME'].str.slice(5, 7).astype(int)   # Extract the month\n",
    "        df2['TIME']  = df2['TIME'].astype(str)\n",
    "        df2['MONTH'] = df2['TIME'].str.slice(5, 7).astype(int)   # Extract the month\n",
    "\n",
    "        vars1 = df1.groupby('MONTH')[0]\n",
    "        vars2 = df2.groupby('MONTH')[0]\n",
    "      \n",
    "        # Loop through each month (1 to 12).\n",
    "        for m in range(1, 13):\n",
    "            value = obs.iloc[s][str(m)]\n",
    "            if value is None or math.isnan(value):  # Skip if the value is None or NaN\n",
    "                 corr_mean = np.nan\n",
    "            else:\n",
    "                bootstrap = []\n",
    "                # print(vars1)\n",
    "                X  = vars1.get_group(m).values\n",
    "                Y  = vars2.get_group(m).values\n",
    "                sort_nan = ~np.logical_or.reduce((np.isnan(X),np.isnan(Y)))\n",
    "                X_m  = X[sort_nan]\n",
    "                Y_m  = Y[sort_nan]\n",
    "                # X_m = vars1[month == m]\n",
    "                # Y_m = vars2[month == m] \n",
    "                \n",
    "                if len(X_m > 1):\n",
    "                    for _ in range(bootsnums):\n",
    "                        # Sample indices with replacement\n",
    "                        idx = np.random.choice(len(X_m), samplenums, replace=True)\n",
    "                        X_sample = X_m[idx]\n",
    "                        Y_sample = Y_m[idx]\n",
    "\n",
    "                        # Compute Pearson correlation for the current month.\n",
    "                        corr = stats.pearsonr(X_sample, Y_sample)[0]\n",
    "                        bootstrap.append(corr)\n",
    "                    corr_mean = np.mean(bootstrap)\n",
    "                else:\n",
    "                    corr_mean = np.nan\n",
    "                    \n",
    "            corr_values.append(corr_mean)\n",
    "        # Store the monthly correlations in a DataFrame.\n",
    "        results[pair_name] = pd.DataFrame([corr_values], columns=columns_mo)\n",
    "\n",
    "   \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f754292a-1c12-4e3e-ab2b-3fb1e22edb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_MI_cesm(s, rad, le, sh, gh, swc, month, samplenums=90):\n",
    "    pairs = {   'swc_rad': ('swc', 'rad'),\n",
    "                'swc_le':  ('swc', 'le'),\n",
    "                'swc_sh':  ('swc', 'sh'),\n",
    "                'swc_gh':  ('swc', 'gh'),\n",
    "                'rad_le':  ('rad', 'le'),\n",
    "                'rad_sh':  ('rad', 'sh'),\n",
    "                'rad_gh':  ('rad', 'gh'),\n",
    "                'le_sh':   ('le', 'sh'),\n",
    "                'le_gh':   ('le', 'gh'),\n",
    "                'sh_gh':   ('sh', 'gh'),}\n",
    "    var_dict = {    'swc': swc,\n",
    "                    'rad': rad,\n",
    "                    'le': le,\n",
    "                    'sh': sh,\n",
    "                    'gh': gh}\n",
    "\n",
    "    # Create month column names as strings '1' to '12'\n",
    "    columns_mo = [str(i) for i in range(1, 13)]\n",
    "    results1 = {}\n",
    "    results2 = {}\n",
    "    results3 = {}\n",
    "    results4 = {}\n",
    "    results5 = {}\n",
    "    \n",
    "    \n",
    "    dir_obs = '/projects/COLA/land/skim/FLUXNET/AMERIbase/'\n",
    "    obs     = pd.read_csv(dir_obs+'obs_chck.csv',na_values=-9999)\n",
    "    \n",
    "    # Loop over each variable pair in the dictionary.\n",
    "    for pair_name, (var1_name, var2_name) in pairs.items():\n",
    "        df1 = var_dict[var1_name].reset_index()\n",
    "        df2 = var_dict[var2_name].reset_index()\n",
    "\n",
    "        nmi_values = []\n",
    "        nmi_nonlinear_values = []\n",
    "        nmi_linear_values = []\n",
    "        te_x_y_values = []\n",
    "        te_y_x_values = []\n",
    "        month_values = []\n",
    "\n",
    "        df1['TIME']  = df1['TIME'].astype(str)\n",
    "        df1['MONTH'] = df1['TIME'].str.slice(5, 7).astype(int)   # Extract the month\n",
    "        df2['TIME']  = df2['TIME'].astype(str)\n",
    "        df2['MONTH'] = df2['TIME'].str.slice(5, 7).astype(int)   # Extract the month\n",
    "\n",
    "        vars1 = df1.groupby('MONTH')[0]\n",
    "        vars2 = df2.groupby('MONTH')[0]\n",
    "      \n",
    "        # Loop through each month (1 to 12).\n",
    "        for m in range(1, 13):\n",
    "            value = obs.iloc[s][str(m)]\n",
    "            if value is None or math.isnan(value):  # Skip if the value is None or NaN\n",
    "                nmi = np.nan\n",
    "                nmi_nonlinear =  np.nan\n",
    "                nmi_linear =  np.nan\n",
    "                te_x_y =  np.nan\n",
    "                te_y_x =  np.nan\n",
    "            else:\n",
    "                # Compute Pearson correlation for the current month.\n",
    "                X  = vars1.get_group(m).values\n",
    "                Y  = vars2.get_group(m).values\n",
    "                sort_nan = ~np.logical_or.reduce((np.isnan(X),np.isnan(Y)))\n",
    "                X  = X[sort_nan]\n",
    "                Y  = Y[sort_nan]\n",
    "                if len(X) > samplenums:\n",
    "                    nmi = calc_MI(X,Y)\n",
    "                    Y_p= calc_quantile_transform(X,Y)\n",
    "                    nmi_nonlinear = calc_MI(X,Y_p)\n",
    "                    nmi_linear    = nmi - nmi_nonlinear\n",
    "                    te_x_y        = calc_transfer_entropy(X,Y)\n",
    "                    te_y_x        = calc_transfer_entropy(Y,X)\n",
    "                else:\n",
    "                    nmi = np.nan\n",
    "                    nmi_nonlinear =  np.nan\n",
    "                    nmi_linear =  np.nan\n",
    "                    te_x_y =  np.nan\n",
    "                    te_y_x =  np.nan\n",
    "                \n",
    "            nmi_values.append(nmi)\n",
    "            nmi_nonlinear_values.append(nmi_nonlinear)\n",
    "            nmi_linear_values.append(nmi_linear)\n",
    "            te_x_y_values.append(te_x_y)\n",
    "            te_y_x_values.append(te_y_x)\n",
    "            \n",
    "        # Store the monthly correlations in a DataFrame.\n",
    "        results1[pair_name] = pd.DataFrame([nmi_values], columns=columns_mo)\n",
    "        results2[pair_name] = pd.DataFrame([nmi_nonlinear_values], columns=columns_mo)\n",
    "        results3[pair_name] = pd.DataFrame([nmi_linear_values], columns=columns_mo)\n",
    "        results4[pair_name] = pd.DataFrame([te_x_y_values], columns=columns_mo)\n",
    "        results5[pair_name] = pd.DataFrame([te_y_x_values], columns=columns_mo)\n",
    "       \n",
    "\n",
    "    # print('done')\n",
    "   \n",
    "    return results1,results2,results3,results4,results5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "12729267-9340-4074-ac50-b32a0f4000e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import math \n",
    "def mk_clm_mo_r(time_scale):\n",
    "    site_list = site_obs\n",
    "    f_start = 0 ; f_stop = len(site_list)\n",
    "    \n",
    "    columns = ['NETRAD_1_A','LE_1_A','H_1_A','G_1_A','SWC_1_A']\n",
    "    pairs = {   'swc_rad': ('swc', 'rad'),\n",
    "                'swc_le':  ('swc', 'le'),\n",
    "                'swc_sh':  ('swc', 'sh'),\n",
    "                'swc_gh':  ('swc', 'gh'),\n",
    "                'rad_le':  ('rad', 'le'),\n",
    "                'rad_sh':  ('rad', 'sh'),\n",
    "                'rad_gh':  ('rad', 'gh'),\n",
    "                'le_sh':   ('le', 'sh'),\n",
    "                'le_gh':   ('le', 'gh'),\n",
    "                'sh_gh':   ('sh', 'gh'),}\n",
    "    \n",
    "    columns_mo = [str(i) for i in range(1, 13)]\n",
    "    results_out = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out2 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out3 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out4 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out5 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out6 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    for s,site in enumerate(site_list[:]):\n",
    "    # for s,site in enumerate(site_list[1:5]):\n",
    "        f_siteid = site_list[s]\n",
    "        \n",
    "        dir_loc = '/projects/COLA/land/skim/FLUXNET/AMERIbase/out_v2/site/'\n",
    "        df_loc  = pd.read_csv(dir_loc+f_siteid+'_site.csv',na_values=-9999)\n",
    "        \n",
    "        lat     = df_loc['Latitude'].item()\n",
    "        if float(lat) < 50:\n",
    "            # print(f_siteid)\n",
    "            dir1= '/projects/COLA/land/skim/FLUXNET/AMERIbase/cesm/clm/out_v2'\n",
    "            model_len = 8761\n",
    "            df1   = pd.read_csv(dir1+'/LE_1_A/'+f_siteid+'.csv',na_values=-9999)[:model_len]\n",
    "            df2   = pd.read_csv(dir1+'/H_1_A/'+f_siteid+'.csv',na_values=-9999)[:model_len]\n",
    "            df3   = pd.read_csv(dir1+'/GH_1_A/'+f_siteid+'.csv',na_values=-9999)[:model_len]\n",
    "            df4   = pd.read_csv(dir1+'/SWC_1_A/'+f_siteid+'.csv',na_values=-9999)[:model_len]\n",
    "            df5   = pd.read_csv(dir1+'/Rn_1_A/'+f_siteid+'.csv',na_values=-9999)[:model_len]\n",
    "\n",
    "            df1['TIME'] = df1['TIME'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
    "            df1.index = pd.PeriodIndex([t.strftime(\"%Y-%m-%d\") for t in df1['TIME']], freq='D')\n",
    "            month = df1.index.month\n",
    "\n",
    "            df1.set_index(\"TIME\", inplace=True)\n",
    "            df2.set_index(\"TIME\", inplace=True)\n",
    "            df3.set_index(\"TIME\", inplace=True)\n",
    "            df4.set_index(\"TIME\", inplace=True)\n",
    "            df5.set_index(\"TIME\", inplace=True)\n",
    "            \n",
    "            # rad = df5['Rn_1_A']\n",
    "            # le  = df1['LE_1_A']\n",
    "            # sh  = df2['H_1_A']\n",
    "            # gh  = df3['GH_1_A']\n",
    "            # swc = df4['SWC_1_A']\n",
    "\n",
    "            le     = running_average(df1['LE_1_A'])\n",
    "            sh     = running_average(df2['H_1_A'])\n",
    "            gh     = running_average(df3['GH_1_A'])\n",
    "            swc    = running_average(df4['SWC_1_A'])\n",
    "            rad    = running_average(df5['Rn_1_A'])  \n",
    "            # print(le)\n",
    "            \n",
    "            le  = pd.Series(le , index=df1.index)\n",
    "            sh  = pd.Series(sh , index=df2.index)\n",
    "            gh  = pd.Series(gh , index=df3.index)\n",
    "            swc = pd.Series(swc, index=df4.index)\n",
    "            rad = pd.Series(rad, index=df5.index)\n",
    "   \n",
    "            # corr = mk_corr_cesm(s, rad, le, sh, gh, swc, month)\n",
    "            mi   = mk_MI_cesm(s, rad, le, sh, gh, swc, month)\n",
    "                \n",
    "            for pair_name in pairs:\n",
    "                # results_out[pair_name] = pd.concat([results_out[pair_name], corr[pair_name]],ignore_index=True)\n",
    "                results_out2[pair_name] = pd.concat([results_out2[pair_name], mi[0][pair_name]],ignore_index=True)\n",
    "                results_out3[pair_name] = pd.concat([results_out3[pair_name], mi[1][pair_name]],ignore_index=True)\n",
    "                results_out4[pair_name] = pd.concat([results_out4[pair_name], mi[2][pair_name]],ignore_index=True)\n",
    "                results_out5[pair_name] = pd.concat([results_out5[pair_name], mi[3][pair_name]],ignore_index=True)\n",
    "                results_out6[pair_name] = pd.concat([results_out6[pair_name], mi[4][pair_name]],ignore_index=True)\n",
    "                \n",
    "\n",
    "    # print('done')\n",
    "    return  results_out2,results_out3,results_out4,results_out5,results_out6    \n",
    "    # print('done')\n",
    "    # return results_out, results_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea48401-316e-4166-ad7d-8f3c5181afdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clm = mk_clm_mo_r('1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5213675-630a-4455-a857-bd900f987a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_amip_mo_r(time_scale):\n",
    "    site_list = site_obs\n",
    "    f_start = 0 ; f_stop = len(site_list)\n",
    "    \n",
    "    columns = ['NETRAD_1_A','LE_1_A','H_1_A','G_1_A','SWC_1_A']\n",
    "    pairs = {   'swc_rad': ('swc', 'rad'),\n",
    "                'swc_le':  ('swc', 'le'),\n",
    "                'swc_sh':  ('swc', 'sh'),\n",
    "                'swc_gh':  ('swc', 'gh'),\n",
    "                'rad_le':  ('rad', 'le'),\n",
    "                'rad_sh':  ('rad', 'sh'),\n",
    "                'rad_gh':  ('rad', 'gh'),\n",
    "                'le_sh':   ('le', 'sh'),\n",
    "                'le_gh':   ('le', 'gh'),\n",
    "                'sh_gh':   ('sh', 'gh'),}\n",
    "    \n",
    "    columns_mo = [str(i) for i in range(1, 13)]\n",
    "    results_out = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out2 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out3 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out4 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out5 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    results_out6 = {pair_name: pd.DataFrame(columns=columns_mo) for pair_name in pairs.keys()}\n",
    "    \n",
    "    for s,site in enumerate(site_list[:]):\n",
    "    # for s,site in enumerate(site_list[1:5]):\n",
    "        f_siteid = site_list[s]\n",
    "        \n",
    "        dir_loc = '/projects/COLA/land/skim/FLUXNET/AMERIbase/out_v2/site/'\n",
    "        df_loc  = pd.read_csv(dir_loc+f_siteid+'_site.csv',na_values=-9999)\n",
    "        \n",
    "        lat     = df_loc['Latitude'].item()\n",
    "        if float(lat) < 50:\n",
    "            # print(f_siteid)\n",
    "            dir1= '/projects/COLA/land/skim/FLUXNET/AMERIbase/cesm/amip/out_v2'\n",
    "            model_len = 8761\n",
    "            df1   = pd.read_csv(dir1+'/LE_1_A/'+f_siteid+'.csv',na_values=-9999)[:model_len]\n",
    "            df2   = pd.read_csv(dir1+'/H_1_A/'+f_siteid+'.csv',na_values=-9999)[:model_len]\n",
    "            df3   = pd.read_csv(dir1+'/GH_1_A/'+f_siteid+'.csv',na_values=-9999)[:model_len]\n",
    "            df4   = pd.read_csv(dir1+'/SWC_1_A/'+f_siteid+'.csv',na_values=-9999)[:model_len]\n",
    "            df5   = pd.read_csv(dir1+'/Rn_1_A/'+f_siteid+'.csv',na_values=-9999)[:model_len]\n",
    "\n",
    "            df1['TIME'] = df1['TIME'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\"))\n",
    "            df1.index = pd.PeriodIndex([t.strftime(\"%Y-%m-%d\") for t in df1['TIME']], freq='D')\n",
    "            month = df1.index.month\n",
    "            \n",
    "            df1.set_index(\"TIME\", inplace=True)\n",
    "            df2.set_index(\"TIME\", inplace=True)\n",
    "            df3.set_index(\"TIME\", inplace=True)\n",
    "            df4.set_index(\"TIME\", inplace=True)\n",
    "            df5.set_index(\"TIME\", inplace=True)\n",
    "            \n",
    "\n",
    "            le     = running_average(df1['LE_1_A'])\n",
    "            sh     = running_average(df2['H_1_A'])\n",
    "            gh     = running_average(df3['GH_1_A'])\n",
    "            swc    = running_average(df4['SWC_1_A'])\n",
    "            rad    = running_average(df5['Rn_1_A'])   \n",
    "\n",
    "            \n",
    "            le  = pd.Series(le , index=df1.index)\n",
    "            sh  = pd.Series(sh , index=df2.index)\n",
    "            gh  = pd.Series(gh , index=df3.index)\n",
    "            swc = pd.Series(swc, index=df4.index)\n",
    "            rad = pd.Series(rad, index=df5.index)\n",
    "            # print(rad,le,sh,gh,swc)\n",
    "\n",
    "            # corr = mk_corr_cesm(s, rad, le, sh, gh, swc, month)\n",
    "            mi   = mk_MI_cesm(s, rad, le, sh, gh, swc, month)\n",
    "                \n",
    "            for pair_name in pairs:\n",
    "                # results_out[pair_name] = pd.concat([results_out[pair_name], corr[pair_name]],ignore_index=True)\n",
    "                results_out2[pair_name] = pd.concat([results_out2[pair_name], mi[0][pair_name]],ignore_index=True)\n",
    "                results_out3[pair_name] = pd.concat([results_out3[pair_name], mi[1][pair_name]],ignore_index=True)\n",
    "                results_out4[pair_name] = pd.concat([results_out4[pair_name], mi[2][pair_name]],ignore_index=True)\n",
    "                results_out5[pair_name] = pd.concat([results_out5[pair_name], mi[3][pair_name]],ignore_index=True)\n",
    "                results_out6[pair_name] = pd.concat([results_out6[pair_name], mi[4][pair_name]],ignore_index=True)\n",
    "                \n",
    "\n",
    "    # print('done')\n",
    "    return  results_out2,results_out3,results_out4,results_out5,results_out6    \n",
    "    # print('done')\n",
    "    # return results_out, results_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc2ec0d-8192-4e7b-8001-53c0c615d7c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "amip= mk_amip_mo_r('1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa01510-7e79-4209-883e-20e7a2c8f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_summary(corr_dict):\n",
    "    summary_dict = {}\n",
    "    for pair_name, df in corr_dict.items():\n",
    "        # Compute quartiles and median for each column and create a summary DataFrame.\n",
    "        summary_df = pd.DataFrame({'Q1': df.quantile(0.25),\n",
    "                                    'median': df.median(),\n",
    "                                    'Q3': df.quantile(0.75)}).T  # Transpose so that the index is ['Q1', 'median', 'Q3'].\n",
    "        summary_dict[pair_name] = summary_df\n",
    "    return summary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a8e2be-8573-4834-913f-4dd310908c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summaries for each correlation dictionary.\n",
    "obs_corr_summary   = compute_summary(obs[0])\n",
    "era_corr_summary   = compute_summary(era[0])\n",
    "merra_corr_summary = compute_summary(merra[0])\n",
    "clm_corr_summary   = compute_summary(clm[0])\n",
    "amip_corr_summary  = compute_summary(amip[0])\n",
    "\n",
    "obs_mi_summary   = compute_summary(obs[1])\n",
    "era_mi_summary   = compute_summary(era[1])\n",
    "merra_mi_summary = compute_summary(merra[1])\n",
    "clm_mi_summary   = compute_summary(clm[1])\n",
    "amip_mi_summary  = compute_summary(amip[1])\n",
    "\n",
    "# To display the summaries, loop through the dictionary and print each one.\n",
    "# for pair_name, summary in obs_corr_summary.items():\n",
    "#     print(f\"Summary for {pair_name}:\")\n",
    "#     print(summary)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485bfb20-2dd4-4eda-99cc-14a559b92474",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_nmi_summary   = compute_summary(obs[0])\n",
    "era_nmi_summary   = compute_summary(era[0])\n",
    "merra_nmi_summary = compute_summary(merra[0])\n",
    "clm_nmi_summary   = compute_summary(clm[0])\n",
    "amip_nmi_summary  = compute_summary(amip[0])\n",
    "obs_nmi_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66b93b-58e5-456b-8023-13e2c8d559a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_nmi_nl_summary   = compute_summary(obs[1])\n",
    "era_nmi_nl_summary   = compute_summary(era[1])\n",
    "merra_nmi_nl_summary = compute_summary(merra[1])\n",
    "clm_nmi_nl_summary   = compute_summary(clm[1])\n",
    "amip_nmi_nl_summary  = compute_summary(amip[1])\n",
    "obs_nmi_nl_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81710954-1fe1-4c4a-94c8-c6ea090abb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_nmi_l_summary   = compute_summary(obs[2])\n",
    "era_nmi_l_summary   = compute_summary(era[2])\n",
    "merra_nmi_l_summary = compute_summary(merra[2])\n",
    "clm_nmi_l_summary   = compute_summary(clm[2])\n",
    "amip_nmi_l_summary  = compute_summary(amip[2])\n",
    "obs_nmi_l_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2391e209-c951-4403-acab-bcc979700274",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_te_x_y_summary   = compute_summary(obs[3])\n",
    "era_te_x_y_summary   = compute_summary(era[3])\n",
    "merra_te_x_y_summary = compute_summary(merra[3])\n",
    "clm_te_x_y_summary   = compute_summary(clm[3])\n",
    "amip_te_x_y_summary  = compute_summary(amip[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d68832-1033-4afa-8bde-347ff81f1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_te_y_x_summary   = compute_summary(obs[4])\n",
    "era_te_y_x_summary   = compute_summary(era[4])\n",
    "merra_te_y_x_summary = compute_summary(merra[4])\n",
    "clm_te_y_x_summary   = compute_summary(clm[4])\n",
    "amip_te_y_x_summary  = compute_summary(amip[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd106da7-2af6-4a30-8a6e-7e30044ce229",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirout='/projects/COLA/land/skim/AMERIbase/out_v2/corr_output/'\n",
    "# datasets = [obs[0],era[0],merra[0]]\n",
    "datasets = [clm[0],amip[0]]\n",
    "# names    = ['obs','era','merra']\n",
    "names    = ['clm','amip']\n",
    "for i in range(0,2):\n",
    "    for pair_name, df in datasets[i].items():\n",
    "        df.to_csv(dirout+names[i]+f'_{pair_name}.csv', index=False,float_format='%.4g', na_rep=-9999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71beb930-4870-4e0e-99bc-717f679c39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /projects/COLA/land/skim/FLUNXET/AMERIbase/out_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25cb71b-6c59-4ac6-856b-22ad06f2478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirout='/projects/COLA/land/skim/FLUXNET/AMERIbase/out_v2/nmi_output_v2/'\n",
    "datasets = [obs[1],era[1],merra[1],clm[1],amip[1]]\n",
    "names    = ['obs','era','merra','clm','amip']\n",
    "for i in range(0,5):\n",
    "    for pair_name, df in datasets[i].items():\n",
    "        df.to_csv(dirout+names[i]+f'_{pair_name}.csv', index=False,float_format='%.4g', na_rep=-9999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37a08a37-cb46-4344-836c-9cbb7c1a8627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/COLA/land/skim/AMERIbase/out_v2\n"
     ]
    }
   ],
   "source": [
    "cd /projects/COLA/land/skim/AMERIbase/out_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "486b6e4e-8094-49ae-bf9a-04fa429fa675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 625\n",
      "-rw-rw----+ 1 skim307 proj-cola 12694 Aug  8 05:05 obs_swc_rad.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12713 Aug  8 05:05 obs_swc_le.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12695 Aug  8 05:05 obs_swc_sh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12716 Aug  8 05:05 obs_swc_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12681 Aug  8 05:05 obs_rad_le.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12694 Aug  8 05:05 obs_rad_sh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12692 Aug  8 05:05 obs_rad_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12698 Aug  8 05:05 obs_le_sh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12697 Aug  8 05:05 obs_le_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12703 Aug  8 05:05 obs_sh_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12753 Aug  8 05:05 era_swc_rad.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12766 Aug  8 05:05 era_swc_le.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12749 Aug  8 05:05 era_swc_sh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12743 Aug  8 05:05 era_swc_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12753 Aug  8 05:05 era_rad_le.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12757 Aug  8 05:05 era_rad_sh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12748 Aug  8 05:05 era_rad_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12756 Aug  8 05:05 era_le_sh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12741 Aug  8 05:05 era_le_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12750 Aug  8 05:05 era_sh_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12743 Aug  8 05:05 merra_swc_rad.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12759 Aug  8 05:05 merra_swc_le.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12749 Aug  8 05:05 merra_swc_sh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12742 Aug  8 05:05 merra_swc_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12748 Aug  8 05:05 merra_rad_le.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12749 Aug  8 05:05 merra_rad_sh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12763 Aug  8 05:05 merra_rad_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12750 Aug  8 05:05 merra_le_sh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12756 Aug  8 05:05 merra_le_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12742 Aug  8 05:05 merra_sh_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12735 Aug  8 05:05 clm_swc_rad.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12738 Aug  8 05:05 clm_swc_le.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12716 Aug  8 05:05 clm_swc_sh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12745 Aug  8 05:05 clm_swc_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12746 Aug  8 05:05 clm_rad_le.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12736 Aug  8 05:05 clm_rad_sh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12739 Aug  8 05:05 clm_rad_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12757 Aug  8 05:05 clm_le_sh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12754 Aug  8 05:05 clm_le_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12741 Aug  8 05:05 clm_sh_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12732 Aug  8 05:05 amip_swc_rad.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12742 Aug  8 05:05 amip_swc_le.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12740 Aug  8 05:05 amip_swc_sh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12726 Aug  8 05:05 amip_swc_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12726 Aug  8 05:05 amip_rad_le.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12748 Aug  8 05:05 amip_rad_sh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12752 Aug  8 05:05 amip_rad_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12757 Aug  8 05:05 amip_le_sh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12735 Aug  8 05:05 amip_le_gh.csv\n",
      "-rw-rw----+ 1 skim307 proj-cola 12739 Aug  8 05:05 amip_sh_gh.csv\n"
     ]
    }
   ],
   "source": [
    "ls -rlt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf80db24-b6ed-4f58-b6c3-3d3d420c626c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/COLA/land/skim/AMERIbase/out_v2/nmi_nonlinear_output\n"
     ]
    }
   ],
   "source": [
    "cd nmi_nonlinear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69fa2348-8076-40dd-becb-1c6f13307b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirout='/projects/COLA/land/skim/FLUXNET/AMERIbase/out_v2/nmi_output_v2/'\n",
    "num=0\n",
    "datasets = [obs[num],era[num],merra[num],clm[num],amip[num]]\n",
    "names    = ['obs','era','merra','clm','amip']\n",
    "for i in range(0,5):\n",
    "    for pair_name, df in datasets[i].items():\n",
    "        df.to_csv(dirout+names[i]+f'_{pair_name}.csv', index=False,float_format='%.4g', na_rep=-9999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b46d8-7ea3-4119-9438-f9314e66a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirout='/projects/COLA/land/skim/FLUXNET/AMERIbase/out_v2/nmi_nonlinear_output_v2/'\n",
    "num=1\n",
    "datasets = [obs[num],era[num],merra[num],clm[num],amip[num]]\n",
    "names    = ['obs','era','merra','clm','amip']\n",
    "for i in range(0,5):\n",
    "    for pair_name, df in datasets[i].items():\n",
    "        df.to_csv(dirout+names[i]+f'_{pair_name}.csv', index=False,float_format='%.4g', na_rep=-9999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f535a05b-5d68-4abf-8ec5-0c363c2ebd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirout='/projects/COLA/land/skim/FLUXNET/AMERIbase/out_v2/nmi_linear_output_v2/'\n",
    "num=2\n",
    "datasets = [obs[num],era[num],merra[num],clm[num],amip[num]]\n",
    "names    = ['obs','era','merra','clm','amip']\n",
    "for i in range(0,5):\n",
    "    for pair_name, df in datasets[i].items():\n",
    "        df.to_csv(dirout+names[i]+f'_{pair_name}.csv', index=False,float_format='%.4g', na_rep=-9999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9916b8c0-9b4b-4314-818e-cfecad725e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirout='/projects/COLA/land/skim/FLUXNET/AMERIbase/out_v2/te_x_y_output_v2/'\n",
    "num=3\n",
    "datasets = [obs[num],era[num],merra[num],clm[num],amip[num]]\n",
    "names    = ['obs','era','merra','clm','amip']\n",
    "for i in range(0,5):\n",
    "    for pair_name, df in datasets[i].items():\n",
    "        df.to_csv(dirout+names[i]+f'_{pair_name}.csv', index=False,float_format='%.4g', na_rep=-9999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691b3cb-b0eb-4d7a-8048-96dd176e6cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirout='/projects/COLA/land/skim/FLUXNET/AMERIbase/out_v2/te_y_x_output_v2/'\n",
    "num=4\n",
    "datasets = [obs[num],era[num],merra[num],clm[num],amip[num]]\n",
    "names    = ['obs','era','merra','clm','amip']\n",
    "for i in range(0,5):\n",
    "    for pair_name, df in datasets[i].items():\n",
    "        df.to_csv(dirout+names[i]+f'_{pair_name}.csv', index=False,float_format='%.4g', na_rep=-9999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91139b77-59ab-42e4-8305-f78293fb4d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
